# GitHub OAuth App credentials
# Create at: https://github.com/settings/developers → New OAuth App
# Callback URL: http://localhost:3000/auth/github/callback
GITHUB_CLIENT_ID=your_client_id_here
GITHUB_CLIENT_SECRET=your_client_secret_here
PORT=3000

# ── GodLocal Core ──────────────────────────────────────────────────────
GODLOCAL_FAST_MODEL=qwen3:1.7b
GODLOCAL_FULL_MODEL=qwen3:8b

# ── GroqCloud (optional — enables ~500 tok/s FAST/FULL tiers) ─────────
# Get key: https://console.groq.com → API Keys → Create API Key
# pip install groq   (once)
GROQ_API_KEY=gsk_...your_key_here...

# ── AirLLM GIANT tier (70B layer-by-layer on 4-8GB VRAM) ─────────────
# pip install airllm   (once, on VPS)
AIRLLM_MODEL_ALIAS=llama-70b     # or qwen-72b, llama-405b
AIRLLM_PRECISION=4bit            # 4bit | 8bit | float16
AIRLLM_MAX_TOKENS=512
AIRLLM_LAYER_CACHE_DIR=~/.cache/airllm

# ── X-ZERO Trading ────────────────────────────────────────────────────
XZERO_PRIVKEY=                   # base58 Solana keypair (required for swaps)
HELIUS_API_KEY=                  # or set XZERO_RPC for custom RPC endpoint

# ── Signal sources (all optional, degrade gracefully) ─────────────────
WHALE_ALERT_API_KEY=
ADSB_EXCHANGE_KEY=
TWITTER_BEARER=

# ── MoonPay ───────────────────────────────────────────────────────────
MOONPAY_API_KEY=
MOONPAY_SECRET_KEY=

# ── GroqCloud model overrides (optional, defaults shown) ──────────────
GROQ_FAST_MODEL=openai/gpt-oss-20b        # ~1000 tok/s, classify/summarize
GROQ_FULL_MODEL=qwen/qwen3-32b            # ~400 tok/s, codegen/analyze
GROQ_PLAN_MODEL=moonshotai/kimi-k2-instruct-0905  # 256k ctx, plan/reason
GROQ_GIANT_MODEL=openai/gpt-oss-120b     # ~500 tok/s, deepest reasoning

# ── Cerebras (~2k–3k tok/s) — public API, faster than Groq ───────────────
CEREBRAS_API_KEY=           # get at inference.cerebras.ai → API Keys
CEREBRAS_FAST_MODEL=llama3.1-8b      # ~3k+ tok/s on micro tasks
CEREBRAS_FULL_MODEL=llama-3.3-70b    # ~2k tok/s on complex tasks

# ── Taalas HC1 (~17k tok/s) — request at taalas.com/api-request-form ────
TAALAS_API_KEY=             # request access: taalas.com/api-request-form
TAALAS_MODEL=meta-llama/Llama-3.1-8B-Instruct  # 17k tok/s on HC1 hardware
TAALAS_BASE_URL=https://api.taalas.com/v1

# ── Claude Code local (free, zero API cost) ───────────────────────────────
# Install CLI: npm install -g @anthropic-ai/claude-code
# Then set ANTHROPIC_BASE_URL to point Claude Code at local Ollama
CLAUDE_LOCAL_ENABLED=true
CLAUDE_LOCAL_MODEL=qwen3:8b           # Ollama model to use for Claude Code
CLAUDE_OLLAMA_URL=http://localhost:11434  # or http://ollama:11434 inside Docker
