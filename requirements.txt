# БОГ || OASIS v6 — Python dependencies

# API server
fastapi>=0.111.0
uvicorn[standard]>=0.29.0

# Config
pydantic>=2.7.0
pydantic-settings>=2.2.0

# LLM
ollama>=0.1.0
groq>=0.13.0             # Groq LPU cloud backend (~1000 tok/s)
cerebras-cloud-sdk>=1.0.0  # Cerebras inference (~3000 tok/s, pip install cerebras-cloud-sdk)
# mlx-lm>=0.12.0          # macOS Apple Silicon only — uncomment if using MLX

# BitNet b1.58 2B — CPU-native 1.58-bit inference (0.4GB RAM, 40% faster)
# Install: pip install llama-cpp-python
# Model:   huggingface-cli download microsoft/bitnet-b1.58-2B-4T --include '*.gguf' --local-dir models/
llama-cpp-python>=0.2.0

# Memory
chromadb>=0.5.0

# Networking
httpx>=0.27.0
websockets>=12.0
requests>=2.31.0

# Docker sandbox
docker>=7.0.0

# Integrations
composio-core>=0.7.0
mcp>=1.0.0

# Connectors / bots
python-telegram-bot>=20.0

# Dev / tests
pytest>=8.0.0
pytest-asyncio>=0.23.0

# Web Agent
duckduckgo-search>=6.2.0
trafilatura>=1.9.0
aiofiles>=23.0.0
aiohttp>=3.9.0

# Optional extras (uncomment if needed)
# kuzu>=0.3.0            # GitNexusMCPConnector graph DB
# sentence-transformers  # GitNexusMCPConnector embeddings
